{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest and Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessed data\n",
    "import pandas as pd\n",
    "\n",
    "X_train_smote = pd.read_csv('X_train_smote_dropped.csv')\n",
    "y_train_smote = pd.read_csv('y_train_smote.csv')['fraudulent']\n",
    "X_train_selected = pd.read_csv('X_train_dropped.csv')\n",
    "y_train = pd.read_csv('y_train.csv')['fraudulent']\n",
    "X_test_selected = pd.read_csv('X_test_dropped.csv')\n",
    "y_test = pd.read_csv('y_test.csv')['fraudulent']\n",
    "\n",
    "X_train_smote = X_train_smote.drop(columns='Unnamed: 0')\n",
    "X_train_selected = X_train_selected.drop(columns='Unnamed: 0')\n",
    "X_test_selected = X_test_selected.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning and Evaluation\n",
    "##### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Best ROC AUC score: 0.9999886552556165\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define a simple grid of hyperparameters\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [200, 500, 1000],  # Number of trees\n",
    "    \"max_depth\": [None, 10, 20],      # Maximum depth of trees\n",
    "    \"min_samples_split\": [2, 5, 10],      # Minimum samples required to split a node\n",
    "    \"min_samples_leaf\": [1, 2, 5]       # Minimum samples required at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rf_smote_model = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring=\"roc_auc\",  # Metric to optimize\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "rf_smote_model.fit(X_train_smote, y_train_smote)\n",
    "best_rf_smote = rf_smote_model.best_estimator_\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", rf_smote_model.best_params_)\n",
    "print(\"Best ROC AUC score:\", rf_smote_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF with SMOTE Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00     13620\n",
      "\n",
      "    accuracy                           1.00     27240\n",
      "   macro avg       1.00      1.00      1.00     27240\n",
      "weighted avg       1.00      1.00      1.00     27240\n",
      "\n",
      "ROC AUC Score: 1.0000\n",
      "Brier Score: 0.0000\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.96      0.73      0.83       182\n",
      "\n",
      "    accuracy                           0.98      3576\n",
      "   macro avg       0.97      0.86      0.91      3576\n",
      "weighted avg       0.98      0.98      0.98      3576\n",
      "\n",
      "ROC AUC Score: 0.8619\n",
      "Brier Score: 0.0154\n",
      "RF original dataset Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00       683\n",
      "\n",
      "    accuracy                           1.00     14303\n",
      "   macro avg       1.00      1.00      1.00     14303\n",
      "weighted avg       1.00      1.00      1.00     14303\n",
      "\n",
      "ROC AUC Score: 1.0000\n",
      "Brier Score: 0.0000\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3394\n",
      "           1       0.98      0.56      0.71       182\n",
      "\n",
      "    accuracy                           0.98      3576\n",
      "   macro avg       0.98      0.78      0.85      3576\n",
      "weighted avg       0.98      0.98      0.97      3576\n",
      "\n",
      "ROC AUC Score: 0.7799\n",
      "Brier Score: 0.0229\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Evaluation\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Random Forest with SMOTE \n",
    "# Train the model with the best parameters on the full dataset\n",
    "best_params = {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
    "best_rf_smote = RandomForestClassifier(**best_params, random_state=42, class_weight='balanced')\n",
    "best_rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate on train and test set\n",
    "rf_smote_y_train_pred = best_rf_smote.predict(X_train_smote)\n",
    "rf_smote_y_test_pred = best_rf_smote.predict(X_test_selected)\n",
    "\n",
    "print(\"RF with SMOTE Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train_smote, rf_smote_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train_smote, rf_smote_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train_smote, rf_smote_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, rf_smote_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, rf_smote_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, rf_smote_y_test_pred):.4f}\")\n",
    "\n",
    "## Random Forest no SMOTE \n",
    "# Train the model with the best parameters on the full dataset\n",
    "best_rf_smote.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate on train and test set\n",
    "rf_y_train_pred = best_rf_smote.predict(X_train_selected)\n",
    "rf_y_test_pred = best_rf_smote.predict(X_test_selected)\n",
    "\n",
    "print(\"RF original dataset Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train, rf_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train, rf_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train, rf_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, rf_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, rf_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, rf_y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pseudolabeling on SMOTE and original dataset on Random Forest model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudolabeling Random Forest Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00     13620\n",
      "\n",
      "    accuracy                           1.00     27240\n",
      "   macro avg       1.00      1.00      1.00     27240\n",
      "weighted avg       1.00      1.00      1.00     27240\n",
      "\n",
      "ROC AUC Score: 1.0000\n",
      "Brier Score: 0.0000\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.97      0.73      0.83       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.98      0.86      0.91      3576\n",
      "weighted avg       0.98      0.99      0.98      3576\n",
      "\n",
      "ROC AUC Score: 0.8648\n",
      "Brier Score: 0.0148\n"
     ]
    }
   ],
   "source": [
    "## Pseudolabeling Random Forest SMOTE \n",
    "# Initialize parameters\n",
    "confidence_threshold = 0.9  # Confidence threshold for pseudolabeling\n",
    "n_iterations = 10  # Reduced iterations for speed\n",
    "top_k = 500  # Max high-confidence samples to include per iteration\n",
    "\n",
    "# Initial training with the labeled training set\n",
    "best_rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Iterative pseudolabeling\n",
    "for i in range(n_iterations):\n",
    "    print(\"Iteration\" + str(i))\n",
    "    test_pred_proba = best_rf_smote.predict_proba(X_test_selected)\n",
    "\n",
    "    # Select top-k confident samples\n",
    "    confident_indices = np.argsort(test_pred_proba.max(axis=1))[-top_k:]\n",
    "    pseudolabeled_data = X_test_selected.iloc[confident_indices]\n",
    "    pseudolabel_targets = test_pred_proba[confident_indices].argmax(axis=1)\n",
    "\n",
    "    # Augment training data\n",
    "    augmented_train_data = np.vstack([X_train_smote, pseudolabeled_data])\n",
    "    augmented_train_labels = np.concatenate([y_train_smote, pseudolabel_targets])\n",
    "\n",
    "    # Retrain with warm_start\n",
    "    best_rf_smote.set_params(warm_start=True, n_estimators=best_rf_smote.n_estimators + 10)\n",
    "    best_rf_smote.fit(augmented_train_data, augmented_train_labels)\n",
    "\n",
    "# Evaluate pseudolabelling result\n",
    "rf_smote_pseudo_y_train_pred = best_rf_smote.predict(X_train_smote)\n",
    "rf_smote_pseudo_y_test_pred = best_rf_smote.predict(X_test_selected)\n",
    "\n",
    "print(\"Pseudolabeling Random Forest Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train_smote, rf_smote_pseudo_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train_smote, rf_smote_pseudo_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train_smote, rf_smote_pseudo_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, rf_smote_pseudo_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, rf_smote_pseudo_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, rf_smote_pseudo_y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:468: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudolabeling Random Forest Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00       683\n",
      "\n",
      "    accuracy                           1.00     14303\n",
      "   macro avg       1.00      1.00      1.00     14303\n",
      "weighted avg       1.00      1.00      1.00     14303\n",
      "\n",
      "ROC AUC Score: 1.0000\n",
      "Brier Score: 0.0000\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3394\n",
      "           1       0.98      0.66      0.79       182\n",
      "\n",
      "    accuracy                           0.98      3576\n",
      "   macro avg       0.98      0.83      0.89      3576\n",
      "weighted avg       0.98      0.98      0.98      3576\n",
      "\n",
      "ROC AUC Score: 0.8321\n",
      "Brier Score: 0.0176\n"
     ]
    }
   ],
   "source": [
    "## Pseudolabeling Random Forest \n",
    "# Initialize parameters\n",
    "confidence_threshold = 0.9  # Confidence threshold for pseudolabeling\n",
    "n_iterations = 10  # Reduced iterations for speed\n",
    "top_k = 500  # Max high-confidence samples to include per iteration\n",
    "\n",
    "# Initial training with the labeled training set\n",
    "best_rf_smote.fit(X_train_selected, y_train)\n",
    "\n",
    "# Iterative pseudolabeling\n",
    "for i in range(n_iterations):\n",
    "    print(\"Iteration\" + str(i))\n",
    "    test_pred_proba = best_rf_smote.predict_proba(X_test_selected)\n",
    "\n",
    "    # Select top-k confident samples\n",
    "    confident_indices = np.argsort(test_pred_proba.max(axis=1))[-top_k:]\n",
    "    pseudolabeled_data = X_test_selected.iloc[confident_indices]\n",
    "    pseudolabel_targets = test_pred_proba[confident_indices].argmax(axis=1)\n",
    "\n",
    "    # Augment training data\n",
    "    augmented_train_data = np.vstack([X_train_selected, pseudolabeled_data])\n",
    "    augmented_train_labels = np.concatenate([y_train, pseudolabel_targets])\n",
    "\n",
    "    # Retrain with warm_start\n",
    "    best_rf_smote.set_params(warm_start=True, n_estimators=best_rf_smote.n_estimators + 10)\n",
    "    best_rf_smote.fit(augmented_train_data, augmented_train_labels)\n",
    "\n",
    "# Evaluate pseudolabelling result\n",
    "rf_pseudo_y_train_pred = best_rf_smote.predict(X_train_selected)\n",
    "rf_pseudo_y_test_pred = best_rf_smote.predict(X_test_selected)\n",
    "\n",
    "print(\"Pseudolabeling Random Forest Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train, rf_pseudo_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train, rf_pseudo_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train, rf_pseudo_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, rf_pseudo_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, rf_pseudo_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, rf_pseudo_y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "125 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "55 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.99792881        nan 0.99782531\n",
      "        nan 0.99780358        nan 0.9978294         nan 0.99777523\n",
      "        nan        nan 0.99782698 0.99784016 0.99779895        nan\n",
      " 0.99792884 0.9977918  0.99782587 0.99782776        nan 0.99783345\n",
      " 0.99779924 0.99781062 0.99782779 0.99792967 0.99781229        nan\n",
      "        nan 0.99792884        nan        nan 0.99792951 0.99792911\n",
      " 0.99782024        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.99782676        nan 0.99792913        nan\n",
      "        nan 0.997929  ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10.488728535883558, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression Hyperparameter Tuning \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# Define the model\n",
    "logreg = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_distributions = {\n",
    "    'C': loguniform(0.01, 1, 10),  # Log-uniform distribution for regularization strength\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Type of regularization\n",
    "    'solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=logreg,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Optimize for AUC score\n",
    "    cv=5,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit to the training data\n",
    "random_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best estimator\n",
    "best_logreg_smote = random_search.best_estimator_\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with SMOTE Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     13620\n",
      "           1       0.99      1.00      0.99     13620\n",
      "\n",
      "    accuracy                           0.99     27240\n",
      "   macro avg       0.99      0.99      0.99     27240\n",
      "weighted avg       0.99      0.99      0.99     27240\n",
      "\n",
      "ROC AUC Score: 0.9941\n",
      "Brier Score: 0.0059\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      3394\n",
      "           1       0.69      0.85      0.76       182\n",
      "\n",
      "    accuracy                           0.97      3576\n",
      "   macro avg       0.84      0.92      0.87      3576\n",
      "weighted avg       0.98      0.97      0.97      3576\n",
      "\n",
      "ROC AUC Score: 0.9157\n",
      "Brier Score: 0.0268\n",
      "Logistic Regression original dataset Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     13620\n",
      "           1       0.56      1.00      0.72       683\n",
      "\n",
      "    accuracy                           0.96     14303\n",
      "   macro avg       0.78      0.98      0.85     14303\n",
      "weighted avg       0.98      0.96      0.97     14303\n",
      "\n",
      "ROC AUC Score: 0.9802\n",
      "Brier Score: 0.0377\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      3394\n",
      "           1       0.52      0.91      0.66       182\n",
      "\n",
      "    accuracy                           0.95      3576\n",
      "   macro avg       0.76      0.93      0.82      3576\n",
      "weighted avg       0.97      0.95      0.96      3576\n",
      "\n",
      "ROC AUC Score: 0.9305\n",
      "Brier Score: 0.0481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Evaluation\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, classification_report\n",
    "\n",
    "## Logistic Regression with SMOTE \n",
    "# Train the model with the best parameters on the full dataset\n",
    "best_logreg_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate on validation set\n",
    "log_smote_y_train_pred = best_logreg_smote.predict(X_train_smote)\n",
    "log_smote_y_test_pred = best_logreg_smote.predict(X_test_selected)\n",
    "\n",
    "print(\"Logistic Regression with SMOTE Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train_smote, log_smote_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train_smote, log_smote_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train_smote, log_smote_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, log_smote_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, log_smote_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, log_smote_y_test_pred):.4f}\")\n",
    "\n",
    "## Random Forest no SMOTE \n",
    "# Train the model with the best parameters on the full dataset\n",
    "best_logreg_smote.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate on train and test set\n",
    "log_y_train_pred = best_logreg_smote.predict(X_train_selected)\n",
    "log_y_test_pred = best_logreg_smote.predict(X_test_selected)\n",
    "\n",
    "print(\"Logistic Regression original dataset Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train, log_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train, log_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train, log_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, log_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, log_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, log_y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pseudolabeling on SMOTE and original dataset on Random Forest model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration9\n",
      "Pseudolabeling Logistic Regression with SMOTE Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     13620\n",
      "           1       0.99      1.00      0.99     13620\n",
      "\n",
      "    accuracy                           0.99     27240\n",
      "   macro avg       0.99      0.99      0.99     27240\n",
      "weighted avg       0.99      0.99      0.99     27240\n",
      "\n",
      "ROC AUC Score: 0.9939\n",
      "Brier Score: 0.0061\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      3394\n",
      "           1       0.69      0.85      0.76       182\n",
      "\n",
      "    accuracy                           0.97      3576\n",
      "   macro avg       0.84      0.92      0.87      3576\n",
      "weighted avg       0.98      0.97      0.97      3576\n",
      "\n",
      "ROC AUC Score: 0.9157\n",
      "Brier Score: 0.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Pseudolabeling Logistic Regression with SMOTE\n",
    "# Initialize parameters\n",
    "confidence_threshold = 0.9  # Confidence threshold for pseudolabeling\n",
    "n_iterations = 10  # Reduced iterations for speed\n",
    "top_k = 500  # Max high-confidence samples to include per iteration\n",
    "\n",
    "# Initial training with the labeled training set\n",
    "best_logreg_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Iterative pseudolabeling\n",
    "for i in range(n_iterations):\n",
    "    print(\"Iteration\" + str(i))\n",
    "    test_pred_proba = best_logreg_smote.predict_proba(X_test_selected)\n",
    "\n",
    "    # Select top-k confident samples\n",
    "    confident_indices = np.argsort(test_pred_proba.max(axis=1))[-top_k:]\n",
    "    pseudolabeled_data = X_test_selected.iloc[confident_indices]\n",
    "    pseudolabel_targets = test_pred_proba[confident_indices].argmax(axis=1)\n",
    "\n",
    "    # Augment training data\n",
    "    augmented_train_data = np.vstack([X_train_smote, pseudolabeled_data])\n",
    "    augmented_train_labels = np.concatenate([y_train_smote, pseudolabel_targets])\n",
    "\n",
    "    # Retrain Logistic Regression on the augmented dataset\n",
    "    best_logreg_smote.fit(augmented_train_data, augmented_train_labels)\n",
    "\n",
    "# Evaluate pseudolabeling result\n",
    "log_smote_pseudo_y_train_pred = best_logreg_smote.predict(X_train_smote)\n",
    "log_smote_pseudo_y_test_pred = best_logreg_smote.predict(X_test_selected)\n",
    "\n",
    "print(\"Pseudolabeling Logistic Regression with SMOTE Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train_smote, log_smote_pseudo_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train_smote, log_smote_pseudo_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train_smote, log_smote_pseudo_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, log_smote_pseudo_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, log_smote_pseudo_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, log_smote_pseudo_y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration9\n",
      "Pseudolabeling Logistic Regression with SMOTE Classification Report:\n",
      "Training:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     13620\n",
      "           1       0.61      1.00      0.76       683\n",
      "\n",
      "    accuracy                           0.97     14303\n",
      "   macro avg       0.80      0.98      0.87     14303\n",
      "weighted avg       0.98      0.97      0.97     14303\n",
      "\n",
      "ROC AUC Score: 0.9818\n",
      "Brier Score: 0.0306\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      3394\n",
      "           1       0.56      0.90      0.69       182\n",
      "\n",
      "    accuracy                           0.96      3576\n",
      "   macro avg       0.78      0.93      0.84      3576\n",
      "weighted avg       0.97      0.96      0.96      3576\n",
      "\n",
      "ROC AUC Score: 0.9292\n",
      "Brier Score: 0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Joseph Nathanael\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Pseudolabeling Logistic Regression\n",
    "# Initialize parameters\n",
    "confidence_threshold = 0.9  # Confidence threshold for pseudolabeling\n",
    "n_iterations = 10  # Reduced iterations for speed\n",
    "top_k = 500  # Max high-confidence samples to include per iteration\n",
    "\n",
    "# Initial training with the labeled training set\n",
    "best_logreg_smote.fit(X_train_selected, y_train)\n",
    "\n",
    "# Iterative pseudolabeling\n",
    "for i in range(n_iterations):\n",
    "    print(\"Iteration\" + str(i))\n",
    "    test_pred_proba = best_logreg_smote.predict_proba(X_test_selected)\n",
    "\n",
    "    # Select top-k confident samples\n",
    "    confident_indices = np.argsort(test_pred_proba.max(axis=1))[-top_k:]\n",
    "    pseudolabeled_data = X_test_selected.iloc[confident_indices]\n",
    "    pseudolabel_targets = test_pred_proba[confident_indices].argmax(axis=1)\n",
    "\n",
    "    # Augment training data\n",
    "    augmented_train_data = np.vstack([X_train_selected, pseudolabeled_data])\n",
    "    augmented_train_labels = np.concatenate([y_train, pseudolabel_targets])\n",
    "\n",
    "    # Retrain Logistic Regression on the augmented dataset\n",
    "    best_logreg_smote.fit(augmented_train_data, augmented_train_labels)\n",
    "\n",
    "# Evaluate pseudolabeling result\n",
    "log_pseudo_y_train_pred = best_logreg_smote.predict(X_train_selected)\n",
    "log_pseudo_y_test_pred = best_logreg_smote.predict(X_test_selected)\n",
    "\n",
    "\n",
    "print(\"Pseudolabeling Logistic Regression with SMOTE Classification Report:\")\n",
    "print(\"Training:\")\n",
    "print(classification_report(y_train, log_pseudo_y_train_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_train, log_pseudo_y_train_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_train, log_pseudo_y_train_pred):.4f}\")\n",
    "print(\"Test:\")\n",
    "print(classification_report(y_test, log_pseudo_y_test_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, log_pseudo_y_test_pred):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, log_pseudo_y_test_pred):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
