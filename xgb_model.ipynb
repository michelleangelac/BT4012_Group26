{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote_dropped =pd.read_csv('X_train_smote_dropped.csv').drop(columns=['Unnamed: 0'])\n",
    "X_train_dropped = pd.read_csv('X_train_dropped.csv').drop(columns=['Unnamed: 0'])\n",
    "X_test_dropped = pd.read_csv('X_test_dropped.csv').drop(columns=['Unnamed: 0']) \n",
    "\n",
    "y_train_smote = pd.read_csv('y_train_smote.csv')['fraudulent']\n",
    "y_test = pd.read_csv('y_test.csv')['fraudulent']\n",
    "y_train = pd.read_csv('y_train.csv')['fraudulent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Hyperparameter tuning using SMOTE Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "Training Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00     13620\n",
      "\n",
      "    accuracy                           1.00     27240\n",
      "   macro avg       1.00      1.00      1.00     27240\n",
      "weighted avg       1.00      1.00      1.00     27240\n",
      "\n",
      "ROC AUC Score: 0.9999999730464615\n",
      "Brier Score: 0.0003258478816547882\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.93      0.79      0.85       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.96      0.89      0.92      3576\n",
      "weighted avg       0.99      0.99      0.99      3576\n",
      "\n",
      "ROC AUC Score: 0.9876737876148602\n",
      "Brier Score: 0.011100783346473223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, brier_score_loss, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=15)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'max_depth': [4, 5],             \n",
    "    'learning_rate': [0.05, 0.1],    \n",
    "    'subsample': [0.8, 1.0],         \n",
    "    'colsample_bytree': [0.8],       \n",
    "    'gamma': [0.1, 0.3],             \n",
    "    'alpha': [0.1, 0.3, 0.5],        \n",
    "    'lambda': [0.1, 0.3, 0.5],       \n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,                            \n",
    "    scoring='roc_auc',                    \n",
    "    cv=3,                                 \n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_smote_dropped, y_train_smote)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "best_model.fit(X_train_smote_dropped, y_train_smote)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, classification_report\n",
    "y_train_pred = best_model.predict(X_train_smote_dropped)\n",
    "y_train_proba = best_model.predict_proba(X_train_smote_dropped)[:, 1]\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(classification_report(y_train_smote, y_train_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_train_smote, y_train_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_train_smote, y_train_proba))\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_dropped)\n",
    "y_test_proba = best_model.predict_proba(X_test_dropped)[:, 1]\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_test_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_test, y_test_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00     13620\n",
      "\n",
      "    accuracy                           1.00     27240\n",
      "   macro avg       1.00      1.00      1.00     27240\n",
      "weighted avg       1.00      1.00      1.00     27240\n",
      "\n",
      "ROC AUC Score: 1.0\n",
      "Brier Score: 0.00030942367333941647\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.93      0.80      0.86       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.96      0.90      0.93      3576\n",
      "weighted avg       0.99      0.99      0.99      3576\n",
      "\n",
      "ROC AUC Score: 0.9876203643145305\n",
      "Brier Score: 0.010758967088551789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_smote), y=y_train_smote)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "sample_weights = np.array([class_weight_dict[label] for label in y_train_smote])\n",
    "\n",
    "mod_xgb_model = xgb.XGBClassifier(**best_params)\n",
    "mod_xgb_model.fit(X_train_smote_dropped, y_train_smote, sample_weight=sample_weights)\n",
    "\n",
    "y_train_pred = mod_xgb_model.predict(X_train_smote_dropped)\n",
    "y_train_proba = mod_xgb_model.predict_proba(X_train_smote_dropped)[:, 1]\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(classification_report(y_train_smote, y_train_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_train_smote, y_train_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_train_smote, y_train_proba))\n",
    "\n",
    "y_test_pred = mod_xgb_model.predict(X_test_dropped)\n",
    "y_test_proba = mod_xgb_model.predict_proba(X_test_dropped)[:, 1]\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_test_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_test, y_test_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Pseudolabelling using SMOTE Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pseudolabeling Iteration 1 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 2 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 3 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 4 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 5 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 6 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 7 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 8 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 9 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 10 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 11 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 12 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 13 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 14 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 15 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 16 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 17 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 18 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 19 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 20 ---\n",
      "\n",
      "--- Final Model Evaluation ---\n",
      "\n",
      "Training Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00     13620\n",
      "\n",
      "    accuracy                           1.00     27240\n",
      "   macro avg       1.00      1.00      1.00     27240\n",
      "weighted avg       1.00      1.00      1.00     27240\n",
      "\n",
      "ROC AUC Score: 0.9999999892185846\n",
      "Brier Score: 0.00038599826443708467\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.92      0.77      0.84       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.95      0.89      0.92      3576\n",
      "weighted avg       0.98      0.99      0.98      3576\n",
      "\n",
      "ROC AUC Score: 0.9884346649225847\n",
      "Brier Score: 0.011701134837374932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, brier_score_loss, classification_report\n",
    "\n",
    "mod_xgb_model_pseudo = xgb.XGBClassifier(**best_params)\n",
    "mod_xgb_model_pseudo.fit(X_train_smote_dropped, y_train_smote, sample_weight=sample_weights)\n",
    "\n",
    "confidence_threshold = 0.9  \n",
    "n_iterations = 20  \n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\n--- Pseudolabeling Iteration {i+1} ---\")\n",
    "\n",
    "    val_pred_proba = mod_xgb_model_pseudo.predict_proba(X_test_dropped)\n",
    "\n",
    "    pseudolabels = np.where(val_pred_proba.max(axis=1) > confidence_threshold, val_pred_proba.argmax(axis=1), -1)\n",
    "\n",
    "    pseudolabeled_data = X_test_dropped[pseudolabels != -1]\n",
    "    pseudolabel_targets = pseudolabels[pseudolabels != -1]\n",
    "\n",
    "    if len(pseudolabeled_data) == 0:\n",
    "        print(\"No high-confidence pseudolabels found. Stopping iteration.\")\n",
    "        break\n",
    "\n",
    "    augmented_train_data = np.vstack([X_train_smote_dropped, pseudolabeled_data])\n",
    "    augmented_train_labels = np.concatenate([y_train_smote, pseudolabel_targets])\n",
    "\n",
    "    mod_xgb_model_pseudo.fit(augmented_train_data, augmented_train_labels)\n",
    "\n",
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "\n",
    "y_train_pred = mod_xgb_model_pseudo.predict(X_train_smote_dropped)\n",
    "y_train_proba = mod_xgb_model_pseudo.predict_proba(X_train_smote_dropped)[:, 1]\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(classification_report(y_train_smote, y_train_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_train_smote, y_train_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_train_smote, y_train_proba))\n",
    "\n",
    "y_test_pred = mod_xgb_model_pseudo.predict(X_test_dropped)\n",
    "y_test_proba = mod_xgb_model_pseudo.predict_proba(X_test_dropped)[:, 1]\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_test_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_test, y_test_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost using Best Hyperparameters on Non-SMOTE Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00       683\n",
      "\n",
      "    accuracy                           1.00     14303\n",
      "   macro avg       1.00      1.00      1.00     14303\n",
      "weighted avg       1.00      1.00      1.00     14303\n",
      "\n",
      "ROC AUC Score: 0.9999998925015534\n",
      "Brier Score: 0.00042806360279484655\n",
      "\n",
      "Validation Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.95      0.75      0.84       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.97      0.88      0.92      3576\n",
      "weighted avg       0.99      0.99      0.98      3576\n",
      "\n",
      "ROC AUC Score: 0.9901037383359127\n",
      "Brier Score: 0.01138209963162096\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.95      0.75      0.84       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.97      0.88      0.92      3576\n",
      "weighted avg       0.99      0.99      0.98      3576\n",
      "\n",
      "ROC AUC Score: 0.9901037383359127\n",
      "Brier Score: 0.01138209963162096\n"
     ]
    }
   ],
   "source": [
    "xgb_model_no_smote = xgb.XGBClassifier(**best_params)\n",
    "sample_weights = np.ones(len(y_train))  \n",
    "xgb_model_no_smote.fit(X_train_dropped, y_train, sample_weight=sample_weights)\n",
    "\n",
    "y_train_pred = xgb_model_no_smote.predict(X_train_dropped)\n",
    "y_train_proba = xgb_model_no_smote.predict_proba(X_train_dropped)[:, 1]\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_train, y_train_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_train, y_train_proba))\n",
    "\n",
    "y_test_pred = xgb_model_no_smote.predict(X_test_dropped)\n",
    "y_test_proba = xgb_model_no_smote.predict_proba(X_test_dropped)[:, 1]\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_test_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_test, y_test_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pseudolabelling using XGBoost on Non-SMOTE Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pseudolabeling Iteration 1 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 2 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 3 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 4 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 5 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 6 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 7 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 8 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 9 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 10 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 11 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 12 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 13 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 14 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 15 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 16 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 17 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 18 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 19 ---\n",
      "\n",
      "--- Pseudolabeling Iteration 20 ---\n",
      "\n",
      "--- Model Evaluation (No smote + Pseudolabelling) ---\n",
      "\n",
      "Training Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13620\n",
      "           1       1.00      1.00      1.00       683\n",
      "\n",
      "    accuracy                           1.00     14303\n",
      "   macro avg       1.00      1.00      1.00     14303\n",
      "weighted avg       1.00      1.00      1.00     14303\n",
      "\n",
      "ROC AUC Score: 0.9999998925015534\n",
      "Brier Score: 0.0004468168592127192\n",
      "\n",
      "Validation Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.97      0.75      0.84       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.98      0.87      0.92      3576\n",
      "weighted avg       0.99      0.99      0.99      3576\n",
      "\n",
      "ROC AUC Score: 0.9902235360396822\n",
      "Brier Score: 0.011620730162902179\n",
      "\n",
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3394\n",
      "           1       0.97      0.75      0.84       182\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.98      0.87      0.92      3576\n",
      "weighted avg       0.99      0.99      0.99      3576\n",
      "\n",
      "ROC AUC Score: 0.9902235360396822\n",
      "Brier Score: 0.011620730162902179\n"
     ]
    }
   ],
   "source": [
    "xgb_model_no_smote_pseudo = xgb.XGBClassifier(**best_params)\n",
    "xgb_model_no_smote_pseudo.fit(X_train_dropped, y_train, sample_weight=sample_weights)\n",
    "\n",
    "confidence_threshold = 0.9  \n",
    "n_iterations = 20  \n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\n--- Pseudolabeling Iteration {i+1} ---\")\n",
    "\n",
    "    val_pred_proba = xgb_model_no_smote_pseudo.predict_proba(X_test_dropped)\n",
    "\n",
    "    pseudolabels = np.where(val_pred_proba.max(axis=1) > confidence_threshold, val_pred_proba.argmax(axis=1), -1)\n",
    "\n",
    "    pseudolabeled_data = X_test_dropped[pseudolabels != -1]\n",
    "    pseudolabel_targets = pseudolabels[pseudolabels != -1]\n",
    "\n",
    "    if len(pseudolabeled_data) == 0:\n",
    "        print(\"No high-confidence pseudolabels found. Stopping iteration.\")\n",
    "        break\n",
    "\n",
    "    augmented_train_data = np.vstack([X_train_dropped, pseudolabeled_data])\n",
    "    augmented_train_labels = np.concatenate([y_train, pseudolabel_targets])\n",
    "\n",
    "    xgb_model_no_smote_pseudo.fit(augmented_train_data, augmented_train_labels)\n",
    "\n",
    "\n",
    "print(\"\\n--- Model Evaluation (No smote + Pseudolabelling) ---\")\n",
    "\n",
    "y_train_pred = xgb_model_no_smote_pseudo.predict(X_train_dropped)\n",
    "y_train_proba = xgb_model_no_smote_pseudo.predict_proba(X_train_dropped)[:, 1]\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_train, y_train_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_train, y_train_proba))\n",
    "\n",
    "y_test_pred = xgb_model_no_smote_pseudo.predict(X_test_dropped)\n",
    "y_test_proba = xgb_model_no_smote_pseudo.predict_proba(X_test_dropped)[:, 1]\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_test_proba))\n",
    "print(\"Brier Score:\", brier_score_loss(y_test, y_test_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
